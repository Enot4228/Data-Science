{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRBQX713AyOY"
   },
   "source": [
    "# Boosting\n",
    "\n",
    "На примерах урока мы увидели, что алгоритм XGBoost работает лучше и быстрее относительно GradientBoosting и AdaBoost. Поэтому в этом домашнем задании предлагаем посмотреть, какие еще интересные методы существуют в библиотеке для реализацией этого алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUWasdEtAyOZ"
   },
   "source": [
    "### Важность признаков\n",
    "\n",
    "Признаки, которые используют для обучения модели, оказывают большое влияние на ее качество. Неинформативные или слабо информативные признаки могут существенно понизить эффективность модели.\n",
    "\n",
    "Отбор признаков – процесс выбора признаков, имеющих наиболее тесные взаимосвязи с целевой переменной. Он позволяет:\n",
    "- уменьшить переобучение: чем меньше избыточных данных, тем меньше возможностей для модели принимать решения на основе «шума»\n",
    "- повысить точность: чем меньше противоречивых данных, тем выше точность\n",
    "- сократить время обучения: чем меньше данных, тем быстрее обучается модель\n",
    "\n",
    "В библиотеке sckit-learn есть раздел feature_selection, который помогает автоматизировать этот процесс на основе методов статистики, линейной алгебры и некоторых специальных алгоритмов, таких как уже знакомый нам PCA - метод главных компонент.\n",
    "\n",
    "Некоторые из таких методов реализованы в библиотеках для конкретных алгоритмов, к которым относится и xgboost.\n",
    "\n",
    "Сгенерируем синтетические данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m_PZVtdLAyOa",
    "outputId": "ca33cfff-37da-4bed-ef2c-d214cfa6cdc2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=15, n_informative=7, \n",
    "                           n_redundant=3, n_repeated=3, random_state=17)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, stratify=y, random_state=17)\n",
    "\n",
    "features = pd.DataFrame(x)\n",
    "targets = pd.Series(y)\n",
    "\n",
    "scat_mrtx = pd.plotting.scatter_matrix(features, c=targets, figsize=(25, 25), marker='o',\n",
    "                                       hist_kwds={'bins': 20}, s=40, alpha=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4Fxn5SmAyOd"
   },
   "source": [
    "Теперь приступим к построению модели. В уроке мы разбирали алгоритм xgboost в интерфейсе библиотеки sklearn: для обучения использовали метод *fit* модели, а для предсказаний - *predict*.\n",
    "\n",
    "Здесь будем использовать интерфейс библиотеки xgboost.\n",
    "\n",
    "В этом интерфейсе для обучения XGBoost данные должны быть представлены в виде объекта DMatrix - внутренней структуры данных, используемой библиотекой  XGBoost, которая оптимизирована как по эффективности работы с памятью, так и по скорости обучения.\n",
    "\n",
    "Также заранее определим параметры алгоритма:\n",
    "- бинарная классификация - 'objective': 'binary:logistic'\n",
    "- глубина деревьев - 'max_depth': 3\n",
    "- шаг градиентного спуска (скорость обучения) - 'eta': 0.1\n",
    "- количество итераций бустинга - num_rounds = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ofz-UYaUAyOe"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test, y_test)\n",
    "\n",
    "params = {'objective': 'binary:logistic',\n",
    "          'max_depth': 3,\n",
    "          'eta': 0.1}\n",
    "\n",
    "num_rounds = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p74BAqoEAyOg"
   },
   "source": [
    "### 1.\n",
    "С помощью метода *train* библиотеки xgboost обучите модель. Передайте в метод вышеопределенные словарь параметров, тренировочные данные в формате DMatrix и число итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FUUdoZFwAyOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMmLTVdAyOj"
   },
   "source": [
    "### 2.\n",
    "В методе *train* есть параметр *evals* - список валидационных наборов данных, для которых будут оцениваться показатели во время обучения. Они помогут нам отслеживать качество модели для каждой итерации.\n",
    "\n",
    "Создайте список из двух кортежей (dtest, 'test') и (dtrain, 'train'). Снова определите метод train аналогично предыдущему пункту, добавив этот список в качестве параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V8ejlm9GAyOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttest-logloss:0.66343\ttrain-logloss:0.65325\n",
      "[1]\ttest-logloss:0.63855\ttrain-logloss:0.62012\n",
      "[2]\ttest-logloss:0.61802\ttrain-logloss:0.59225\n",
      "[3]\ttest-logloss:0.59999\ttrain-logloss:0.56655\n",
      "[4]\ttest-logloss:0.58840\ttrain-logloss:0.54443\n",
      "[5]\ttest-logloss:0.57064\ttrain-logloss:0.52526\n",
      "[6]\ttest-logloss:0.55664\ttrain-logloss:0.50836\n",
      "[7]\ttest-logloss:0.54734\ttrain-logloss:0.49023\n",
      "[8]\ttest-logloss:0.53869\ttrain-logloss:0.47380\n",
      "[9]\ttest-logloss:0.53187\ttrain-logloss:0.45862\n",
      "[10]\ttest-logloss:0.51961\ttrain-logloss:0.44627\n",
      "[11]\ttest-logloss:0.51053\ttrain-logloss:0.43366\n",
      "[12]\ttest-logloss:0.50065\ttrain-logloss:0.42058\n",
      "[13]\ttest-logloss:0.49537\ttrain-logloss:0.40988\n",
      "[14]\ttest-logloss:0.48768\ttrain-logloss:0.40141\n",
      "[15]\ttest-logloss:0.48188\ttrain-logloss:0.39375\n",
      "[16]\ttest-logloss:0.47386\ttrain-logloss:0.38457\n",
      "[17]\ttest-logloss:0.47245\ttrain-logloss:0.37775\n",
      "[18]\ttest-logloss:0.46815\ttrain-logloss:0.37156\n",
      "[19]\ttest-logloss:0.46157\ttrain-logloss:0.36540\n",
      "[20]\ttest-logloss:0.45955\ttrain-logloss:0.36028\n",
      "[21]\ttest-logloss:0.45567\ttrain-logloss:0.35374\n",
      "[22]\ttest-logloss:0.45710\ttrain-logloss:0.35012\n",
      "[23]\ttest-logloss:0.45638\ttrain-logloss:0.34643\n",
      "[24]\ttest-logloss:0.45142\ttrain-logloss:0.34182\n",
      "[25]\ttest-logloss:0.45005\ttrain-logloss:0.33812\n",
      "[26]\ttest-logloss:0.44747\ttrain-logloss:0.33303\n",
      "[27]\ttest-logloss:0.44413\ttrain-logloss:0.32893\n",
      "[28]\ttest-logloss:0.43616\ttrain-logloss:0.32148\n",
      "[29]\ttest-logloss:0.43798\ttrain-logloss:0.31880\n",
      "[30]\ttest-logloss:0.43369\ttrain-logloss:0.31390\n",
      "[31]\ttest-logloss:0.43260\ttrain-logloss:0.31191\n",
      "[32]\ttest-logloss:0.43350\ttrain-logloss:0.30984\n",
      "[33]\ttest-logloss:0.43232\ttrain-logloss:0.30801\n",
      "[34]\ttest-logloss:0.43330\ttrain-logloss:0.30514\n",
      "[35]\ttest-logloss:0.43342\ttrain-logloss:0.30232\n",
      "[36]\ttest-logloss:0.43217\ttrain-logloss:0.30061\n",
      "[37]\ttest-logloss:0.42574\ttrain-logloss:0.29598\n",
      "[38]\ttest-logloss:0.42486\ttrain-logloss:0.29293\n",
      "[39]\ttest-logloss:0.42467\ttrain-logloss:0.29044\n",
      "[40]\ttest-logloss:0.42145\ttrain-logloss:0.28597\n",
      "[41]\ttest-logloss:0.41913\ttrain-logloss:0.28292\n",
      "[42]\ttest-logloss:0.41832\ttrain-logloss:0.28084\n",
      "[43]\ttest-logloss:0.41763\ttrain-logloss:0.27923\n",
      "[44]\ttest-logloss:0.41669\ttrain-logloss:0.27790\n",
      "[45]\ttest-logloss:0.41648\ttrain-logloss:0.27474\n",
      "[46]\ttest-logloss:0.41704\ttrain-logloss:0.27301\n",
      "[47]\ttest-logloss:0.41654\ttrain-logloss:0.27160\n",
      "[48]\ttest-logloss:0.41646\ttrain-logloss:0.26978\n",
      "[49]\ttest-logloss:0.41577\ttrain-logloss:0.26862\n",
      "[50]\ttest-logloss:0.41310\ttrain-logloss:0.26581\n",
      "[51]\ttest-logloss:0.40768\ttrain-logloss:0.26021\n",
      "[52]\ttest-logloss:0.40373\ttrain-logloss:0.25609\n",
      "[53]\ttest-logloss:0.40482\ttrain-logloss:0.25450\n",
      "[54]\ttest-logloss:0.40340\ttrain-logloss:0.25277\n",
      "[55]\ttest-logloss:0.40286\ttrain-logloss:0.25074\n",
      "[56]\ttest-logloss:0.40201\ttrain-logloss:0.24810\n",
      "[57]\ttest-logloss:0.40153\ttrain-logloss:0.24720\n",
      "[58]\ttest-logloss:0.40270\ttrain-logloss:0.24582\n",
      "[59]\ttest-logloss:0.40083\ttrain-logloss:0.24371\n"
     ]
    }
   ],
   "source": [
    "evals = [(dtest, 'test'), (dtrain, 'train')]\n",
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds, evals=evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAuRFGCPAyOm"
   },
   "source": [
    "### 3.\n",
    "В качестве метрики оценки важности признаков в XGBoost используется F-score, которая вычисляется на основе того, как часто делалось разбиение по данному признаку.\n",
    "\n",
    "Используйте метод plot_importance библиотеки XGBoost. Передайте туда в качестве параметра xgb_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pVfCI2zfAyOn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23f4cd011c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI6kkR0IAyOq"
   },
   "source": [
    "По полученным данным можно посмотреть, какие признаки являются излишними и их можно убрать, а какие наиболее важны при обучении модели, т.е. используются чаще всего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhjl-S23AyOr"
   },
   "source": [
    "### Несбалансированные выборки\n",
    "\n",
    "Мы уже упоминали о том, что работа с несбалансированными выборками несколько отличается от работы с идеальными датасетами. В XGBoost есть возможность несколько упростить эту работу: \n",
    "- задать большие веса некоторым объектам при инициализации DMatrix\n",
    "- контролировать соотношение числа объектов разных классов с помощью параметра set_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7WtGpyuAyOr"
   },
   "source": [
    "Сгенерируем несбалансированную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N-BF3BssAyOr",
    "outputId": "be614580-e552-41c0-90db-f7044b3e0aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 104 positive instances\n"
     ]
    }
   ],
   "source": [
    "x, y = make_classification(n_samples=1000, n_features=7, n_informative=3, n_redundant=3, \n",
    "                           n_classes=2, weights=[.9, .1], random_state=20)\n",
    "\n",
    "print(f'There are {sum(y)} positive instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnCnslvjAyOv"
   },
   "source": [
    "### 4.\n",
    "Разбейте выборку на тренировочную и тестовую, соблюдая стратификацию по *y* и размер тестового датасета 0.3. Создайте объекты DMatrix для тренировочной и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "we7wd-rQAyOw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BDyKDN9uAyO0"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, stratify=y, random_state=17)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD9q0ogkAyO4"
   },
   "source": [
    "Зададим параметры для алгоритма и количество итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qfltg1nxAyO5"
   },
   "outputs": [],
   "source": [
    "params = {'objective': 'binary:logistic', \n",
    "          'max_depth': 1, \n",
    "          'silent': 1, \n",
    "          'eta': 1}\n",
    "\n",
    "num_rounds = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xndygs2aAyPA"
   },
   "source": [
    "### 5.\n",
    "С помощью метода train обучите модель. С помощью метода predict получите предсказания для тестовых данных. Так как алгоритм возвращает вероятности, получите бинарную матрицу значений этих вероятностей, элементы которой при полученной вероятности > 0.5 равны True, а при вероятности <= 0.5 равны False. Выведите эту матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "efqPQJBDAyPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:39:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:39:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds)\n",
    "xgb_model_predict = xgb_model.predict(dtest)\n",
    "predictions = [round(value) for value in xgb_model_predict]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7pSgFnLAyPG"
   },
   "source": [
    "### 6.\n",
    "Выведите матрицу ошибок, точность и полноту для полученных предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hNqCdKNiAyPH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yczK3j6zAyPL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица ошибок:\n",
      " [[262   7]\n",
      " [ 16  15]]\n",
      "Точность: 0.6818181818181818\n",
      "Полнота: 0.4838709677419355\n"
     ]
    }
   ],
   "source": [
    "conf_mtrx = confusion_matrix(y_test, predictions)\n",
    "print(f'Матрица ошибок:\\n {conf_mtrx}')\n",
    "precision = precision_score(y_test, predictions)\n",
    "print(f'Точность: {precision}')\n",
    "recall = recall_score(y_test, predictions)\n",
    "print(f'Полнота: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a2ydscAyPO"
   },
   "source": [
    "Видно, что полнота гораздо ниже точности, т.е. алгоритм плохо распознает объекты класса, который представлен меньшим количеством экземпляров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N9CT2zLAyPO"
   },
   "source": [
    "Теперь зададим вручную веса для экземпляров классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FJsDoqsaAyPP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.zeros(len(y_train))\n",
    "weights[y_train == 0] = 1\n",
    "weights[y_train == 1] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI6T-lS2AyPR"
   },
   "source": [
    "### 7.\n",
    "Повторите то же, что делали ранее: создайте объекты DMatrix - для тренировочных данных укажите веса, а для тестовых оставьте только сами данные; обучите модель и получите ее предсказания так же, как и в предыдущем случае. Выведите матрицу ошибок, точность и полноту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dvLHkI46AyPS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:55:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Матрица ошибок:\n",
      " [[254  15]\n",
      " [  8  23]]\n",
      "Точность: 0.6052631578947368\n",
      "Полнота: 0.7419354838709677\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(data=x_train, label=y_train, weight=weights)\n",
    "dtest = xgb.DMatrix(x_test, y_test)\n",
    "\n",
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds)\n",
    "xgb_model_predict = xgb_model.predict(dtest)\n",
    "predictions = [round(value) for value in xgb_model_predict]\n",
    "conf_mtrx = confusion_matrix(y_test, predictions)\n",
    "print(f'Матрица ошибок:\\n {conf_mtrx}')\n",
    "precision = precision_score(y_test, predictions)\n",
    "print(f'Точность: {precision}')\n",
    "recall = recall_score(y_test, predictions)\n",
    "print(f'Полнота: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HErMGHkrAyPT"
   },
   "source": [
    "Можем увидеть, что значение полноты увеличилось, но при этом точность уменьшилась. Если нам важны обе метрики, то пригодится параметр scale_pos_weight в XGBoost.\n",
    "\n",
    "### 8.\n",
    "Снова создайте объекты DMatrix, как в первом случае (без задания весов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U9epTz89AyPU"
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwJvz9CtAyPV"
   },
   "source": [
    "### 9.\n",
    "Инициализируйте параметр scale_pos_weight как соотношение числа объектов двух классов. Для этого число объектов класса 0 разделите на число объектов класса 1. Добавьте в словарь параметров алгоритма params пару с ключом 'scale_pos_weight' и значением, равным полученному соотношению классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aIbO61viAyPW"
   },
   "outputs": [],
   "source": [
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in y:\n",
    "    if i == 0:\n",
    "        count_0 += 1\n",
    "    if i == 1:\n",
    "        count_1 += 1\n",
    "scale_pos_weight = count_0 / count_1\n",
    "params = {'objective': 'binary:logistic', \n",
    "          'max_depth': 1, \n",
    "          'silent': 1, \n",
    "          'eta': 1,\n",
    "          'scale_pos_weight': scale_pos_weight}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftccBOCmAyPY"
   },
   "source": [
    "### 10.\n",
    "Обучите модель с параметрами params и получите ее предсказания для тестовой выборки. Выведите матрицу ошибок, точность и полноту.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HzjcOMB_AyPZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:58:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:58:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Матрица ошибок:\n",
      " [[239  30]\n",
      " [  6  25]]\n",
      "Точность: 0.45454545454545453\n",
      "Полнота: 0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds)\n",
    "xgb_model_predict = xgb_model.predict(dtest)\n",
    "predictions = [round(value) for value in xgb_model_predict]\n",
    "conf_mtrx = confusion_matrix(y_test, predictions)\n",
    "print(f'Матрица ошибок:\\n {conf_mtrx}')\n",
    "precision = precision_score(y_test, predictions)\n",
    "print(f'Точность: {precision}')\n",
    "recall = recall_score(y_test, predictions)\n",
    "print(f'Полнота: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSk7-YCjAyPb"
   },
   "source": [
    "Значение параметра scale_pos_weight надо выбирать в зависимости от желаемого соотношения между точностью и полнотой. Например, если нам нужны примерно одинаковые значения precision и recall, в данном случае следует уменьшить полученное значение параметра в несколько раз."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "jun_ml_extra_tech_boost-hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
